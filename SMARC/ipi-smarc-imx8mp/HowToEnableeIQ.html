<!DOCTYPE html>
<html>
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
  <link rel="icon" href="https://xfii.b-cdn.net/uploads/i-pi-smarc/5ieu2go.jpg">
  <title>ADLINK Industrial Pi Wiki</title>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Canonical links -->
  <link rel="canonical" href="https://docs.ipi.wiki/ipi-smarc-imx8mp/HowToEnableeIQ.html">
  <!-- Alternative links -->
  
    
  
  <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
  <link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
  <style>
    .pace .pace-progress {
       background: #1E92FB;
       height: 2px;
     }
    .pace .pace-progress-inner {
       box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB;
     }
    .pace .pace-activity {
       border-top-color: #1E92FB;
       border-left-color: #1E92FB;
    }
  </style>
  <!-- Icon -->
  <link rel="Shortcut Icon" href="/SMARC/images/favicon.ico">
  <!-- CSS -->

  <script
    src="https://s3.pstatp.com/cdn/expire-1-M/jquery/3.3.1/jquery.min.js"
    crossorigin="anonymous"></script>

 <!-- Hotjar Tracking Code for https://ipi.wiki -->
  <script>
      (function(h,o,t,j,a,r){
          h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
          h._hjSettings={hjid:1081538,hjsv:6};
          a=o.getElementsByTagName('head')[0];
          r=o.createElement('script');r.async=1;
          r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
          a.appendChild(r);
      })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
  </script>

  <!-- build:css build/css/navy.css -->
  
<link rel="stylesheet" href="/SMARC/css/navy.css">

  <!-- endbuild -->
  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/SMARC//css/font/fontawesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css">
  <!-- RSS -->
  <link rel="alternate" href="/SMARC/atom.xml" title="ADLINK Industrial Pi Wiki" type="application/atom+xml">
  <!-- Open Graph -->
  <meta name="description" content="How to enable eIQ on IMX8MPThis document provides detailed instructions for enabling eIQ on I-Pi SMARC IMX8M plus.  TensorFlowLite  OpenCV   Prerequisites Download the prebuilt full Yocto Image (2G&amp;#x">
<meta property="og:type" content="website">
<meta property="og:title" content="ADLINK Industrial Pi Wiki">
<meta property="og:url" content="https://docs.ipi.wiki/ipi-smarc-imx8mp/HowToEnableeIQ.html">
<meta property="og:site_name" content="ADLINK Industrial Pi Wiki">
<meta property="og:description" content="How to enable eIQ on IMX8MPThis document provides detailed instructions for enabling eIQ on I-Pi SMARC IMX8M plus.  TensorFlowLite  OpenCV   Prerequisites Download the prebuilt full Yocto Image (2G&amp;#x">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://docs.ipi.wiki/ipi-smarc-imx8mp/HowToEnableeIQ.assets/GraceHopper.jpg">
<meta property="og:image" content="https://docs.ipi.wiki/ipi-smarc-imx8mp/HowToEnableeIQ.assets/ObjectDetectionImage.jpg">
<meta property="og:image" content="https://docs.ipi.wiki/ipi-smarc-imx8mp/HowToEnableeIQ.assets/ObjectDetectionWebcam.jpg">
<meta property="article:published_time" content="2022-03-21T03:41:54.779Z">
<meta property="article:modified_time" content="2022-03-21T03:41:54.770Z">
<meta property="article:author" content="ADLINK Technology Inc. All Rights Reserved.">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://docs.ipi.wiki/ipi-smarc-imx8mp/HowToEnableeIQ.assets/GraceHopper.jpg">
  <!-- Google Analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-161199547-2', 'auto');
  ga('send', 'pageview');
</script>

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <header id="header" class="header-wrapper">
  <div id="header-inner" class="inner wrapper">
    <h1 id="logo-wrap">
      <a href="/" id="logo">Hexo</a>
    </h1>
    <nav id="main-nav">
      <a class="navbar-brand" href="https://www.ipi.wiki/" rel="external nofollow noreferrer" target="_self">
      <img src="https://xfii.b-cdn.net/uploads/i-pi-smarc/5ieu2go.jpg" class="img-fluid" style="height:27px;top: 8px;position: relative;/* display: inline-block; */" alt="">  </a>
      <a href="https://www.ipi.wiki/" rel="external nofollow noreferrer" class="main-nav-link" target="_self"> Home</a>
      <div style="display:inline-block"><div class="btn-group"><a href="/SMARC/ipi-smarc-imx8mp/" class="main-nav-link current header">menu.ipi-smarc-imx8mp</a><a href="/SMARC/ipi-smarc-px30/" class="main-nav-link  header">menu.ipi-smarc-px30</a></div>
      <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/adlink" class="main-nav-link"><i class="fa fa-github-alt"></i> GitHub</a>
      <a href="https://www.ipi.wiki/community/forum" rel="external nofollow noreferrer" class="main-nav-link" target="_self"> Community </a>    
    </nav>
	
	<div id = "changeurl">
	    <script>window.onload = function() {parent.postMessage( {msg: '?page=HowToEnableeIQ.html#HowToEnableeIQ.html'}, '*');}</script>

	</div>
	

	


    <a id="mobile-nav-toggle">
      <span class="mobile-nav-toggle-bar"></span>
      <span class="mobile-nav-toggle-bar"></span>
      <span class="mobile-nav-toggle-bar"></span>
    </a>
  </div>
</header>

    <div id="content-wrap">
  <div id="content" class="wrapper">
    <div id="content-inner">
      <article class="article-container" itemscope itemtype="http://schema.org/Article">
        <div class="article-inner">
          <div class="article">
            <div class="inner">
              <header class="article-header">
                <h1 class="article-title" itemprop="name"></h1>
                <a href="" class="article-edit-link" title="Improve this doc"><i class="fa fa-pencil"></i></a>
              </header>
              <div class="article-content bullets" itemprop="articleBody">
                <h1 id="How-to-enable-eIQ-on-IMX8MP" class="article-heading"><a href="#How-to-enable-eIQ-on-IMX8MP" class="headerlink" title="How to enable eIQ on IMX8MP"></a>How to enable eIQ on IMX8MP<a class="article-anchor" href="#How-to-enable-eIQ-on-IMX8MP" aria-hidden="true"></a></h1><p>This document provides detailed instructions for enabling eIQ on I-Pi SMARC IMX8M plus.</p>
<ul>
<li><p><a href="#Enabling-NPU-and-CPU-on-TensorFlowLite">TensorFlowLite</a></p>
</li>
<li><p><a href="#Enabling-CPU-on-OpenCV">OpenCV</a></p>
</li>
</ul>
<h2 id="Prerequisites" class="article-heading"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a><strong>Prerequisites</strong><a class="article-anchor" href="#Prerequisites" aria-hidden="true"></a></h2><ul>
<li><p>Download the prebuilt full Yocto Image (2G&#x2F;4G). Use any of the following methods to flash the Yocto Image on the IMX8MP,</p>
<ul>
<li>Flashing the image in SD card <a href="HowToFlashImageSD.html">click here</a>.</li>
<li>Flashing the image in eMMC <a href="HowToFlashImageeMMC.html">click here</a>.</li>
<li>Flashing the image in eMMC using UUU tool <a href="HowToFlashImageeMMCUsingUUUTool.html">click here</a>.</li>
</ul>
</li>
<li><p>Once flashed the Yocto Image on IMX8MP turn on it and go to the Yocto terminal which is located on your left corner top side of the screen.</p>
</li>
<li><p>According to the NXP source by far the NPU only enabled on armNN, ONNX, TensorFlowLite and DeepViewRT rest of them run on CPU only.</p>
</li>
</ul>
<h2 id="Enabling-NPU-and-CPU-on-TensorFlowLite" class="article-heading"><a href="#Enabling-NPU-and-CPU-on-TensorFlowLite" class="headerlink" title="Enabling NPU and CPU on TensorFlowLite"></a>Enabling NPU and CPU on TensorFlowLite<a class="article-anchor" href="#Enabling-NPU-and-CPU-on-TensorFlowLite" aria-hidden="true"></a></h2><p>TensorFlow Lite is an open-source software library focused on running machine learning models and embedded devices. It enables on-device machine learning inference with low latency and small binary size.</p>
<h4 id="Features" class="article-heading"><a href="#Features" class="headerlink" title="Features"></a>Features<a class="article-anchor" href="#Features" aria-hidden="true"></a></h4><ul>
<li>TensorFlow Lite v2.5.0</li>
<li>Multithreaded computation with acceleration using Arm Neon SIMD instructions on Cortex-A cores.</li>
<li>Parallel computation using GPU&#x2F;NPU hardware acceleration.</li>
<li>C++ and Python API (supported Python version 3).</li>
</ul>
<p>A Yocto Linux BSP Image with machine learning layer included by default contains a simple preinstalled example called ‘label_image’ usable with image classification models. The example file is located at: <code>/usr/bin/tensorflow-lite-2.5.0/examples</code>.</p>
<p>Go to:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /usr/bin/tensorflow-lite-2.5.0/examples</span><br></pre></td></tr></table></figure>


<img src="HowToEnableeIQ.assets\GraceHopper.jpg" alt="SMARC-module-logo_500px" style="zoom:67%; margin-left: auto; margin-right: auto; display: block; "> 

<p>This is a sample image we used to test the CPU and NPU. You can use your own image but it must be in (.bmp) format.</p>
<p>To run the example with mobilenet model on the CPU, use the following command,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./label_image -m mobilenet_v1_1.0_224_quant.tflite -i grace_hopper.bmp -l labels.txt</span><br></pre></td></tr></table></figure>

<p>The output of a successful classification for the ‘grace_hopper.bmp’ input image is as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Loaded model mobilenet_v1_1.0_224_quant.tflite</span><br><span class="line">resolved reporter</span><br><span class="line">invoked</span><br><span class="line">average time: 39.271 ms</span><br><span class="line">0.780392: 653 military uniform</span><br><span class="line">0.105882: 907 Windsor tie</span><br><span class="line">0.0156863: 458 bow tie</span><br><span class="line">0.0117647: 466 bulletproof vest</span><br><span class="line">0.00784314: 835 suit</span><br></pre></td></tr></table></figure>

<p>To run the example application on the CPU with using the XNNPACK delegate, use the -x 1 switch,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./label_image -m mobilenet_v1_1.0_224_quant.tflite -i grace_hopper.bmp -l labels.txt -x 1</span><br></pre></td></tr></table></figure>

<p>To run the example with the same model on the GPU&#x2F;NPU hardware accelerator, add the -a 1 (for NNAPI Delegate) or -V 1 (for VX Delegate) command line argument. To differentiate between the 3D GPU and the NPU, use the USE_GPU_INFERENCE switch. For example, to run the model accelerated on the NPU hardware using NNAPI Delegate, use this command:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">USE_GPU_INFERENCE=0 ./label_image -m mobilenet_v1_1.0_224_quant.tflite -i grace_hopper.bmp -l labels.txt -a 1</span><br></pre></td></tr></table></figure>

<p>The output with NPU acceleration enabled should be as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Loaded model mobilenet_v1_1.0_224_quant.tflite</span><br><span class="line">resolved reporter</span><br><span class="line">INFO: Created TensorFlow Lite delegate for NNAPI.</span><br><span class="line">Applied NNAPI delegate.</span><br><span class="line">invoked</span><br><span class="line">average time: 2.967 ms</span><br><span class="line">0.74902: 653 military uniform</span><br><span class="line">0.121569: 907 Windsor tie</span><br><span class="line">0.0196078: 458 bow tie</span><br><span class="line">0.0117647: 466 bulletproof vest</span><br><span class="line">0.00784314: 835 suit</span><br></pre></td></tr></table></figure>
<h3 id="Running-Benchmark-Applications" class="article-heading"><a href="#Running-Benchmark-Applications" class="headerlink" title="Running Benchmark Applications"></a>Running Benchmark Applications<a class="article-anchor" href="#Running-Benchmark-Applications" aria-hidden="true"></a></h3><p>A Yocto Linux BSP image with machine learning layer included by default contains a pre-installed benchmarking application. It performs a simple TensorFlow Lite model inference and prints benchmarking information. The application binary file is located at: &#x2F;usr&#x2F;bin&#x2F;tensorflow-lite-2.5.0&#x2F;examples.</p>
<p>Go to the directory:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /usr/bin/tensorflow-lite-2.5.0/examples</span><br></pre></td></tr></table></figure>

<p>To run the benchmark with computation on CPU, use the following command:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite</span><br></pre></td></tr></table></figure>

<p>You can optionally specify the number of threads with the –num_threads&#x3D;X parameter to run the inference on multiple cores. For highest performance, set X to the number of cores available.</p>
<p>The output of the benchmarking application should be similar to:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">STARTING!</span><br><span class="line">Duplicate flags: num_threads</span><br><span class="line">Min num runs: [50]</span><br><span class="line">Min runs duration (seconds): [1]</span><br><span class="line">Max runs duration (seconds): [150]</span><br><span class="line">Inter-run delay (seconds): [-1]</span><br><span class="line">Num threads: [1]</span><br><span class="line">Use caching: [0]</span><br><span class="line">Benchmark name: []</span><br><span class="line">Output prefix: []</span><br><span class="line">Min warmup runs: [1]</span><br><span class="line">Min warmup runs duration (seconds): [0.5]</span><br><span class="line">Graph: [mobilenet_v1_1.0_224_quant.tflite]</span><br><span class="line">Input layers: []</span><br><span class="line">Input shapes: []</span><br><span class="line">Input value ranges: []</span><br><span class="line">Input layer values files: []</span><br><span class="line">Allow fp16 : [0]</span><br><span class="line">Require full delegation : [0]</span><br><span class="line">Enable op profiling: [0]</span><br><span class="line">Max profiling buffer entries: [1024]</span><br><span class="line">CSV File to export profiling data to: []</span><br><span class="line">Enable platform-wide tracing: [0]</span><br><span class="line">#threads used for CPU inference: [1]</span><br><span class="line">Max number of delegated partitions : [0]</span><br><span class="line">Min nodes per partition : [0]</span><br><span class="line">Loaded model mobilenet_v1_1.0_224_quant.tflite</span><br><span class="line">The input model file size (MB): 4.27635</span><br><span class="line">Initialized session in 93.252ms.</span><br><span class="line">Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding</span><br><span class="line">150 seconds.</span><br><span class="line">count=4 first=147477 curr=140410 min=140279 max=147477 avg=142382 std=2971</span><br><span class="line">Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding</span><br><span class="line">150 seconds.</span><br><span class="line">count=50 first=140422 curr=140269 min=140269 max=140532 avg=140391 std=67</span><br><span class="line">Inference timings in us: Init: 93252, First inference: 147477, Warmup (avg): 142382, Inference</span><br><span class="line">(avg): 140391</span><br><span class="line">Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the</span><br><span class="line">actual memory footprint of the model at runtime. Take the information at your discretion.</span><br><span class="line">Peak memory footprint (MB): init=3.14062 overall=10.043</span><br></pre></td></tr></table></figure>

<p>To run the inference using the XNNPACK delegate, add the –use_xnnpack&#x3D;true switch:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --use_xnnpack=true</span><br></pre></td></tr></table></figure>

<p>To run the inference using the GPU&#x2F;NPU hardware accelerator, add the –use_nnapi&#x3D;true (for NNAPI Delegate) or – use_vxdelegate&#x3D;true (for VX Delegate) switch:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --use_nnapi=true</span><br></pre></td></tr></table></figure>

<p>The output with GPU&#x2F;NPU module acceleration enabled should be similar to:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">STARTING!</span><br><span class="line">Duplicate flags: num_threads</span><br><span class="line">Min num runs: [50]</span><br><span class="line">Min runs duration (seconds): [1]</span><br><span class="line">Max runs duration (seconds): [150]</span><br><span class="line">Inter-run delay (seconds): [-1]</span><br><span class="line">Num threads: [1]</span><br><span class="line">Use caching: [0]</span><br><span class="line">Benchmark name: []</span><br><span class="line">Output prefix: []</span><br><span class="line">Min warmup runs: [1]</span><br><span class="line">Min warmup runs duration (seconds): [0.5]</span><br><span class="line">Graph: [mobilenet_v1_1.0_224_quant.tflite]</span><br><span class="line">Input layers: []</span><br><span class="line">Input shapes: []</span><br><span class="line">Input value ranges: []</span><br><span class="line">Input layer values files: []</span><br><span class="line">Allow fp16 : [0]</span><br><span class="line">Require full delegation : [0]</span><br><span class="line">Enable op profiling: [0]</span><br><span class="line">Max profiling buffer entries: [1024]</span><br><span class="line">CSV File to export profiling data to: []</span><br><span class="line">Enable platform-wide tracing: [0]</span><br><span class="line">#threads used for CPU inference: [1]</span><br><span class="line">Max number of delegated partitions : [0]</span><br><span class="line">Min nodes per partition : [0]</span><br><span class="line">Loaded model mobilenet_v1_1.0_224_quant.tflite</span><br><span class="line">INFO: Created TensorFlow Lite delegate for NNAPI.</span><br><span class="line">Applied NNAPI delegate, and the model graph will be completely executed w/ the delegate.</span><br><span class="line">The input model file size (MB): 4.27635</span><br><span class="line">Initialized session in 18.648ms.</span><br><span class="line">Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding</span><br><span class="line">150 seconds.</span><br><span class="line">count=1 curr=5969598</span><br><span class="line">Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding</span><br><span class="line">150 seconds.</span><br><span class="line">count=306 first=3321 curr=3171 min=3161 max=3321 avg=3188.46 std=18</span><br><span class="line">Inference timings in us: Init: 18648, First inference: 5969598, Warmup (avg): 5.9696e+06, Inference</span><br><span class="line">(avg): 3188.46</span><br><span class="line">Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the</span><br><span class="line">actual memory footprint of the model at runtime. Take the information at your discretion.</span><br><span class="line">Peak memory footprint (MB): init=7.60938 overall=33.7773</span><br></pre></td></tr></table></figure>

<h2 id="Enabling-CPU-on-OpenCV" class="article-heading"><a href="#Enabling-CPU-on-OpenCV" class="headerlink" title="Enabling CPU on OpenCV"></a>Enabling CPU on OpenCV<a class="article-anchor" href="#Enabling-CPU-on-OpenCV" aria-hidden="true"></a></h2><p>OpenCV is an open-source computer vision library and one of its modules, called ML, provides traditional machine learning algorithms. OpenCV offers a unified solution for both neural network inference (DNN module) and classic machine learning algorithms (ML module).</p>
<h3 id="Features-1" class="article-heading"><a href="#Features-1" class="headerlink" title="Features"></a>Features<a class="article-anchor" href="#Features-1" aria-hidden="true"></a></h3><ul>
<li>OpenCV 4.5.2</li>
<li>C++ and Python API (supported Python version 3)</li>
<li>Only CPU computation is supported</li>
<li>Input image or live camera (webcam) is supported</li>
</ul>
<p>OpenCV DNN demos (binaries) are located at: &#x2F;usr&#x2F;share&#x2F;OpenCV&#x2F;samples&#x2F;bin.</p>
<p>Input data, and model configurations are located at: &#x2F;usr&#x2F;share&#x2F;opencv4&#x2F;testdata&#x2F;dnn.</p>
<p>To check the OpenCV sample algorithms, we don’t need to install anything new because everything is already installed. Before running the DNN model, you have to copy all the content from the below link, create a file called model.yaml, and paste all the content into it. Copy the classes from the following link as well. Using a USB pen drive, copy the model.yaml file and classes file, then connect the USB pen drive to the carrier module.</p>
<p>To download model.yaml click here(<a target="_blank" rel="noopener external nofollow noreferrer" href="https://raw.githubusercontent.com/opencv/opencv/4.5.2/samples/dnn/models.yml">https://raw.githubusercontent.com/opencv/opencv/4.5.2/samples/dnn/models.yml</a>)</p>
<p>To download classes for the model click here(<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/arunponnusamy/object-detection-opencv/blob/master/yolov3.txt">https://github.com/arunponnusamy/object-detection-opencv/blob/master/yolov3.txt</a>)</p>
<p>To copy the model.yaml file from the USB pen drive to yocto follow the commands:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df -h</span><br><span class="line"></span><br><span class="line">cd /run/media/sda1 (or) sdb1</span><br><span class="line"></span><br><span class="line">cp model.yaml /usr/share/OpenCV/samples/bin (for model.yaml)</span><br><span class="line"></span><br><span class="line">cp object_detection_classes_yolov3.txt /usr/share/opencv4/testdata/dnn (for classes)</span><br></pre></td></tr></table></figure>

<h3 id="YOLO-object-detection-example" class="article-heading"><a href="#YOLO-object-detection-example" class="headerlink" title="YOLO object detection example"></a>YOLO object detection example<a class="article-anchor" href="#YOLO-object-detection-example" aria-hidden="true"></a></h3><p>Running the C++ example with image input from the default location:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/share/OpenCV/samples/data/dnn</span><br></pre></td></tr></table></figure>


<p>Executing with image or video file:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./example_dnn_object_detection --config=[PATH-TO-DARKNET]/cfg/yolo.cfg --model=[PATH-TO-DARKNET]/yolo.weights --classes=[PATH-TO-DARKNET]/object_detection_classes_yolov3.txt --width=416 --height=416 --scale=0.00392 --input=[PATH-TO-IMAGE-OR-VIDEO-FILE] –rgb</span><br></pre></td></tr></table></figure>


<p>For our case,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./example_dnn_object_detection --config=/usr/share/opencv4/testdata/dnn/yolov3.cfg --model=/usr/share/opencv4/testdata/dnn/yolov3.weights –-classes=/usr/share/opencv4/testdata/dnn/object_detection_classes_yolov3.txt --width=416 --height=416 --scale=0.00392 --input=/usr/share/opencv4/testdata/dnn/dog416.png –rgb</span><br></pre></td></tr></table></figure>

<img src="HowToEnableeIQ.assets\ObjectDetectionImage.jpg" alt="SMARC-module-logo_500px" style="zoom:67%; margin-left: auto; margin-right: auto; display: block; "> 

<p>Executing with live webcam:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./example_dnn_object_detection --config=[PATH-TO-DARKNET]/cfg/yolo.cfg --model=[PATH-TO-DARKNET]/yolo.weights --classes=[PATH-TO-DARKNET]/object_detection_classes_yolov3.txt --width=416 --height=416 --scale=0.00392 --device=[CAMERA_DEV_NUMBER] –rgb</span><br></pre></td></tr></table></figure>


<p>For our case,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./example_dnn_object_detection --config=/usr/share/opencv4/testdata/dnn/yolov3.cfg --model=/usr/share/opencv4/testdata/dnn/yolov3.weights –-classes=/usr/share/opencv4/testdata/dnn/object_detection_classes_yolov3.txt --width=416 --height=416 --scale=0.00392 --device=1 –rgb</span><br></pre></td></tr></table></figure>


<img src="HowToEnableeIQ.assets\ObjectDetectionWebcam.jpg" alt="SMARC-module-logo_500px" style="zoom:67%; margin-left: auto; margin-right: auto; display: block; "> 


              </div>
              <footer class="article-footer">
                <time class="article-footer-updated" datetime="2022-03-21T03:41:54.770Z" itemprop="dateModified">Last updated: 2022-03-21</time>
                <a href="AIInferencingOnNPU.html" class="article-footer-prev" title="AI Inferencing"><i class="fa fa-chevron-left"></i><span>Prev</span></a><a href="CanderaCGIStudioPage.html" class="article-footer-next" title="CANDERA CGI Studio"><span>Next</span><i class="fa fa-chevron-right"></i></a>
              </footer>
              
            </div>
          </div>
          <aside id="article-toc" role="navigation">
            <div id="article-toc-inner">
              <div class="article-toc-border">
                <strong class="sidebar-title">Contents</strong>
                <ol class="toc"><li class="toc-item toc-level-1"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#How-to-enable-eIQ-on-IMX8MP'}, '*');" class="toc-link" href="#How-to-enable-eIQ-on-IMX8MP"><span class="toc-text">How to enable eIQ on IMX8MP</span></a><ol class="toc-childbvdf"><li class="toc-item toc-level-2"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#Prerequisites'}, '*');" class="toc-link" href="#Prerequisites"><span class="toc-text">Prerequisites</span></a></li><li class="toc-item toc-level-2"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#Enabling-NPU-and-CPU-on-TensorFlowLite'}, '*');" class="toc-link" href="#Enabling-NPU-and-CPU-on-TensorFlowLite"><span class="toc-text">Enabling NPU and CPU on TensorFlowLite</span></a><ol class="toc-childbvdf"><li class="toc-item toc-level-4"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#Features'}, '*');" class="toc-link" href="#Features"><span class="toc-text">Features</span></a></li></ol></li><li class="toc-item toc-level-3"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#Running-Benchmark-Applications'}, '*');" class="toc-link" href="#Running-Benchmark-Applications"><span class="toc-text">Running Benchmark Applications</span></a></li></ol></li><li class="toc-item toc-level-2"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#Enabling-CPU-on-OpenCV'}, '*');" class="toc-link" href="#Enabling-CPU-on-OpenCV"><span class="toc-text">Enabling CPU on OpenCV</span></a><ol class="toc-childbvdf"><li class="toc-item toc-level-3"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#Features-1'}, '*');" class="toc-link" href="#Features-1"><span class="toc-text">Features</span></a></li><li class="toc-item toc-level-3"><a onclick="parent.postMessage( {msg: '?page=HowToEnableeIQ.html#YOLO-object-detection-example'}, '*');" class="toc-link" href="#YOLO-object-detection-example"><span class="toc-text">YOLO object detection example</span></a></li></ol></li></ol></li></ol>
                <a href="#" id="article-toc-top">Back to Top</a>
              </div>
            </div>
          </aside>
        </div>
      </article>
      <aside id="sidebar" role="navigation">
  <div class="inner">
    <strong class="sidebar-title"><details open><summary>Getting Started</summary><a href="index.html" class="sidebar-link first">Introduction</a><a href="UnBoxing.html" class="sidebar-link">UnBoxing</a><a href="UserInterfaces.html" class="sidebar-link">User Interfaces</a><a href="HowToSetup.html" class="sidebar-link">How to Setup</a></details></strong><strong class="sidebar-title"><details open><summary>Hardware Documentation</summary><a href="ModuleIntroduction.html" class="sidebar-link first">Module Introduction</a><a href="CarrierIntroduction.html" class="sidebar-link">Carrier Introduction</a></details></strong><strong class="sidebar-title"><details open><summary>Software Documentation</summary><a href="YoctoImages.html" class="sidebar-link first">Yocto</a><a href="UbuntuImages.html" class="sidebar-link">Ubuntu</a><a href="DebianImages.html" class="sidebar-link">Debian</a><a href="AndroidImages.html" class="sidebar-link">Android</a><a href="HowToFlashImageSD" class="sidebar-link">Prepare your SD Card</a><a href="HowToFlashImageeMMC.html" class="sidebar-link">Prepare your eMMC</a><a href="HowToFlashImageeMMCUsingUUUTool.html" class="sidebar-link">Enabling UUU Tool with eMMC</a></details></strong><strong class="sidebar-title"><details open><summary>Software Guideline</summary><a href="HowToBuildYocto.html" class="sidebar-link first">How to Build Yocto</a><a href="HowToBuildUbuntu.html" class="sidebar-link">How to Build Ubuntu</a><a href="ExampleHowto.html" class="sidebar-link">Examples & How-to</a></details></strong><strong class="sidebar-title"><details open><summary>AI Inferencing on NPU</summary><a href="AIInferencingOnNPU.html" class="sidebar-link first">AI Inferencing</a><a href="HowToEnableeIQ.html" class="sidebar-link current">How to Enable eIQ</a></details></strong><strong class="sidebar-title"><details open><summary>Software Partners</summary><a href="CanderaCGIStudioPage.html" class="sidebar-link first">CANDERA CGI Studio</a></details></strong>
  </div>
</aside>
    </div>
  </div>
</div>

    <footer id="footer" class="wrapperforfooter">
  <div class="inner">
    <div class="grid grid--no-gutters small--text-center grid--footer-float-right">
  <div class="grid__item site-footer__payment-icons" style="display:none;">
      <span class="visually-hidden">Payment methods</span>
      <ul class="payment-icons list--inline site-footer__icon-list"><li class="payment-icon">
            <svg class="icon icon--full-color" viewBox="0 0 38 24" xmlns="http://www.w3.org/2000/svg" width="38" height="24" role="img" aria-labelledby="pi-paypal"><title id="pi-paypal">PayPal</title><path opacity=".07" d="M35 0H3C1.3 0 0 1.3 0 3v18c0 1.7 1.4 3 3 3h32c1.7 0 3-1.3 3-3V3c0-1.7-1.4-3-3-3z"></path><path fill="#fff" d="M35 1c1.1 0 2 .9 2 2v18c0 1.1-.9 2-2 2H3c-1.1 0-2-.9-2-2V3c0-1.1.9-2 2-2h32"></path><path fill="#003087" d="M23.9 8.3c.2-1 0-1.7-.6-2.3-.6-.7-1.7-1-3.1-1h-4.1c-.3 0-.5.2-.6.5L14 15.6c0 .2.1.4.3.4H17l.4-3.4 1.8-2.2 4.7-2.1z"></path><path fill="#3086C8" d="M23.9 8.3l-.2.2c-.5 2.8-2.2 3.8-4.6 3.8H18c-.3 0-.5.2-.6.5l-.6 3.9-.2 1c0 .2.1.4.3.4H19c.3 0 .5-.2.5-.4v-.1l.4-2.4v-.1c0-.2.3-.4.5-.4h.3c2.1 0 3.7-.8 4.1-3.2.2-1 .1-1.8-.4-2.4-.1-.5-.3-.7-.5-.8z"></path><path fill="#012169" d="M23.3 8.1c-.1-.1-.2-.1-.3-.1-.1 0-.2 0-.3-.1-.3-.1-.7-.1-1.1-.1h-3c-.1 0-.2 0-.2.1-.2.1-.3.2-.3.4l-.7 4.4v.1c0-.3.3-.5.6-.5h1.3c2.5 0 4.1-1 4.6-3.8v-.2c-.1-.1-.3-.2-.5-.2h-.1z"></path></svg>
          </li></ul>
    </div></div><div class="grid__item  small--one-whole site-footer-item-tall"><ul class="list--inline site-footer__social-icons social-icons site-footer__icon-list"><li class="social-icons__item">
                  <a class="social-icons__link" target="_blank" href="https://www.facebook.com/ADLINKTECH/" rel="external nofollow noreferrer" aria-describedby="a11y-external-message"><svg aria-hidden="true" focusable="false" role="presentation" class="icon icon-facebook" viewBox="0 0 20 20"><path d="M18.05.811q.439 0 .744.305t.305.744v16.637q0 .439-.305.744t-.744.305h-4.732v-7.221h2.415l.342-2.854h-2.757v-1.83q0-.659.293-1t1.073-.342h1.488V3.762q-.976-.098-2.171-.098-1.634 0-2.635.964t-1 2.72V9.47H7.951v2.854h2.415v7.221H1.413q-.439 0-.744-.305t-.305-.744V1.859q0-.439.305-.744T1.413.81H18.05z"></path></svg><span class="icon__fallback-text">Facebook</span>
                  </a>
                </li><li class="social-icons__item">
                  <a class="social-icons__link" target="_blank" href="https://twitter.com/i_smarc" rel="external nofollow noreferrer" aria-describedby="a11y-external-message"><svg aria-hidden="true" focusable="false" role="presentation" class="icon icon-twitter" viewBox="0 0 20 20"><path d="M19.551 4.208q-.815 1.202-1.956 2.038 0 .082.02.255t.02.255q0 1.589-.469 3.179t-1.426 3.036-2.272 2.567-3.158 1.793-3.963.672q-3.301 0-6.031-1.773.571.041.937.041 2.751 0 4.911-1.671-1.284-.02-2.292-.784T2.456 11.85q.346.082.754.082.55 0 1.039-.163-1.365-.285-2.262-1.365T1.09 7.918v-.041q.774.408 1.773.448-.795-.53-1.263-1.396t-.469-1.864q0-1.019.509-1.997 1.487 1.854 3.596 2.924T9.81 7.184q-.143-.509-.143-.897 0-1.63 1.161-2.781t2.832-1.151q.815 0 1.569.326t1.284.917q1.345-.265 2.506-.958-.428 1.386-1.732 2.18 1.243-.163 2.262-.611z"></path></svg><span class="icon__fallback-text">Twitter</span>
                  </a>
                </li><li class="social-icons__item">
                  <a class="social-icons__link" target="_blank" href="https://www.youtube.com/channel/UC5N0wFxt_mPucg7CC2sZSLQ" rel="external nofollow noreferrer" aria-describedby="a11y-external-message"><svg aria-hidden="true" focusable="false" role="presentation" class="icon icon-youtube" viewBox="0 0 21 20"><path d="M-.196 15.803q0 1.23.812 2.092t1.977.861h14.946q1.165 0 1.977-.861t.812-2.092V3.909q0-1.23-.82-2.116T17.539.907H2.593q-1.148 0-1.969.886t-.82 2.116v11.894zm7.465-2.149V6.058q0-.115.066-.18.049-.016.082-.016l.082.016 7.153 3.806q.066.066.066.164 0 .066-.066.131l-7.153 3.806q-.033.033-.066.033-.066 0-.098-.033-.066-.066-.066-.131z"></path></svg><span class="icon__fallback-text">YouTube</span>
                  </a>
                </li></ul></div><div class="grid__item medium-up--hide one-half small--one-whole"><div class="grid__item site-footer__payment-icons" style="display:none;">
      <span class="visually-hidden">Payment methods</span>
      <ul class="payment-icons list--inline site-footer__icon-list"><li class="payment-icon">
            <svg class="icon icon--full-color" viewBox="0 0 38 24" xmlns="http://www.w3.org/2000/svg" width="38" height="24" role="img" aria-labelledby="pi-paypal"><title id="pi-paypal">PayPal</title><path opacity=".07" d="M35 0H3C1.3 0 0 1.3 0 3v18c0 1.7 1.4 3 3 3h32c1.7 0 3-1.3 3-3V3c0-1.7-1.4-3-3-3z"></path><path fill="#fff" d="M35 1c1.1 0 2 .9 2 2v18c0 1.1-.9 2-2 2H3c-1.1 0-2-.9-2-2V3c0-1.1.9-2 2-2h32"></path><path fill="#003087" d="M23.9 8.3c.2-1 0-1.7-.6-2.3-.6-.7-1.7-1-3.1-1h-4.1c-.3 0-.5.2-.6.5L14 15.6c0 .2.1.4.3.4H17l.4-3.4 1.8-2.2 4.7-2.1z"></path><path fill="#3086C8" d="M23.9 8.3l-.2.2c-.5 2.8-2.2 3.8-4.6 3.8H18c-.3 0-.5.2-.6.5l-.6 3.9-.2 1c0 .2.1.4.3.4H19c.3 0 .5-.2.5-.4v-.1l.4-2.4v-.1c0-.2.3-.4.5-.4h.3c2.1 0 3.7-.8 4.1-3.2.2-1 .1-1.8-.4-2.4-.1-.5-.3-.7-.5-.8z"></path><path fill="#012169" d="M23.3 8.1c-.1-.1-.2-.1-.3-.1-.1 0-.2 0-.3-.1-.3-.1-.7-.1-1.1-.1h-3c-.1 0-.2 0-.2.1-.2.1-.3.2-.3.4l-.7 4.4v.1c0-.3.3-.5.6-.5h1.3c2.5 0 4.1-1 4.6-3.8v-.2c-.1-.1-.3-.2-.5-.2h-.1z"></path></svg>
          </li></ul>
    </div></div><div class="grid__item small--one-whole  site-footer-item-align-center">
        <small class="site-footer__copyright-content">© 2021, <a href="/" title="">I-Pi SMARC</a></small>
        <small class="site-footer__copyright-content site-footer__copyright-content--powered-by"><a target="_blank" rel="nofollow noopener" href="https://www.shopify.com?utm_campaign=poweredby&amp;utm_medium=shopify&amp;utm_source=onlinestore" aria-describedby="a11y-new-window-external-message"> </a></small>
      </div>
    </div>
  </div>
</footer>

  </div>
  <div id="mobile-nav-dimmer"></div>
  <nav id="mobile-nav">
  <div id="mobile-nav-inner">
    <ul id="mobile-nav-list">
      <a href="/SMARC/ipi-smarc-imx8mp/" class="mobile-nav-link current header">menu.ipi-smarc-imx8mp</a><a href="/SMARC/ipi-smarc-px30/" class="mobile-nav-link  header">menu.ipi-smarc-px30</a>
      <a href="https://github.com/adlink" class="mobile-nav-link" rel="external" target="_blank">GitHub</a>
    </ul>
    
      <strong class="mobile-nav-title"><details open><summary>Getting Started</summary><a href="index.html" class="mobile-nav-link first">Introduction</a><a href="UnBoxing.html" class="mobile-nav-link">UnBoxing</a><a href="UserInterfaces.html" class="mobile-nav-link">User Interfaces</a><a href="HowToSetup.html" class="mobile-nav-link">How to Setup</a></details></strong><strong class="mobile-nav-title"><details open><summary>Hardware Documentation</summary><a href="ModuleIntroduction.html" class="mobile-nav-link first">Module Introduction</a><a href="CarrierIntroduction.html" class="mobile-nav-link">Carrier Introduction</a></details></strong><strong class="mobile-nav-title"><details open><summary>Software Documentation</summary><a href="YoctoImages.html" class="mobile-nav-link first">Yocto</a><a href="UbuntuImages.html" class="mobile-nav-link">Ubuntu</a><a href="DebianImages.html" class="mobile-nav-link">Debian</a><a href="AndroidImages.html" class="mobile-nav-link">Android</a><a href="HowToFlashImageSD" class="mobile-nav-link">Prepare your SD Card</a><a href="HowToFlashImageeMMC.html" class="mobile-nav-link">Prepare your eMMC</a><a href="HowToFlashImageeMMCUsingUUUTool.html" class="mobile-nav-link">Enabling UUU Tool with eMMC</a></details></strong><strong class="mobile-nav-title"><details open><summary>Software Guideline</summary><a href="HowToBuildYocto.html" class="mobile-nav-link first">How to Build Yocto</a><a href="HowToBuildUbuntu.html" class="mobile-nav-link">How to Build Ubuntu</a><a href="ExampleHowto.html" class="mobile-nav-link">Examples & How-to</a></details></strong><strong class="mobile-nav-title"><details open><summary>AI Inferencing on NPU</summary><a href="AIInferencingOnNPU.html" class="mobile-nav-link first">AI Inferencing</a><a href="HowToEnableeIQ.html" class="mobile-nav-link current">How to Enable eIQ</a></details></strong><strong class="mobile-nav-title"><details open><summary>Software Partners</summary><a href="CanderaCGIStudioPage.html" class="mobile-nav-link first">CANDERA CGI Studio</a></details></strong>
    
  </div>
  

</nav>
  <!-- Scripts -->
<!-- build:js build/js/main.js -->

<script src="/SMARC/js/lang_select.js"></script>


<script src="/SMARC/js/toc.js"></script>


<script src="/SMARC/js/mobile_nav.js"></script>

<!-- endbuild -->

<!-- Algolia -->


</body>
</html>
